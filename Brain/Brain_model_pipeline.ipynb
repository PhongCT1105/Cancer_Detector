{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cancer_type</th>\n",
       "      <th>type</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
       "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brain</td>\n",
       "      <td>astrocytoma</td>\n",
       "      <td>9.556081</td>\n",
       "      <td>5.526080</td>\n",
       "      <td>3.650048</td>\n",
       "      <td>5.056509</td>\n",
       "      <td>2.785774</td>\n",
       "      <td>6.570793</td>\n",
       "      <td>6.135645</td>\n",
       "      <td>3.029876</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091654</td>\n",
       "      <td>10.993969</td>\n",
       "      <td>12.576917</td>\n",
       "      <td>12.313470</td>\n",
       "      <td>2.599384</td>\n",
       "      <td>2.440502</td>\n",
       "      <td>1.982966</td>\n",
       "      <td>1.797714</td>\n",
       "      <td>1.964544</td>\n",
       "      <td>2.370085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brain</td>\n",
       "      <td>ependymoma</td>\n",
       "      <td>12.643347</td>\n",
       "      <td>8.408933</td>\n",
       "      <td>7.893075</td>\n",
       "      <td>8.082575</td>\n",
       "      <td>4.091167</td>\n",
       "      <td>9.377509</td>\n",
       "      <td>6.116171</td>\n",
       "      <td>6.018412</td>\n",
       "      <td>...</td>\n",
       "      <td>13.289336</td>\n",
       "      <td>12.742540</td>\n",
       "      <td>14.198576</td>\n",
       "      <td>13.909929</td>\n",
       "      <td>5.403896</td>\n",
       "      <td>4.932116</td>\n",
       "      <td>3.942146</td>\n",
       "      <td>4.306064</td>\n",
       "      <td>4.841028</td>\n",
       "      <td>4.758554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>8.523559</td>\n",
       "      <td>6.446557</td>\n",
       "      <td>6.110491</td>\n",
       "      <td>6.915887</td>\n",
       "      <td>2.375614</td>\n",
       "      <td>7.489329</td>\n",
       "      <td>4.136052</td>\n",
       "      <td>4.581049</td>\n",
       "      <td>...</td>\n",
       "      <td>12.085742</td>\n",
       "      <td>11.495483</td>\n",
       "      <td>13.630228</td>\n",
       "      <td>13.335569</td>\n",
       "      <td>8.594401</td>\n",
       "      <td>6.919923</td>\n",
       "      <td>7.472751</td>\n",
       "      <td>2.284780</td>\n",
       "      <td>2.585936</td>\n",
       "      <td>2.410203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brain</td>\n",
       "      <td>glioblastoma</td>\n",
       "      <td>12.180322</td>\n",
       "      <td>8.993027</td>\n",
       "      <td>8.008274</td>\n",
       "      <td>9.493953</td>\n",
       "      <td>4.454340</td>\n",
       "      <td>9.052279</td>\n",
       "      <td>6.331221</td>\n",
       "      <td>6.812413</td>\n",
       "      <td>...</td>\n",
       "      <td>11.998082</td>\n",
       "      <td>11.357289</td>\n",
       "      <td>13.492403</td>\n",
       "      <td>13.329294</td>\n",
       "      <td>5.252382</td>\n",
       "      <td>4.974248</td>\n",
       "      <td>4.039301</td>\n",
       "      <td>3.655452</td>\n",
       "      <td>4.483640</td>\n",
       "      <td>4.882925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>11.196461</td>\n",
       "      <td>5.821402</td>\n",
       "      <td>5.456620</td>\n",
       "      <td>8.542700</td>\n",
       "      <td>3.245398</td>\n",
       "      <td>8.368442</td>\n",
       "      <td>6.583196</td>\n",
       "      <td>4.639983</td>\n",
       "      <td>...</td>\n",
       "      <td>12.833154</td>\n",
       "      <td>12.251775</td>\n",
       "      <td>14.016996</td>\n",
       "      <td>13.851353</td>\n",
       "      <td>10.713623</td>\n",
       "      <td>5.923269</td>\n",
       "      <td>8.745234</td>\n",
       "      <td>3.344703</td>\n",
       "      <td>3.553666</td>\n",
       "      <td>3.650059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54677 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cancer_type          type  1007_s_at   1053_at    117_at    121_at  \\\n",
       "0       brain   astrocytoma   9.556081  5.526080  3.650048  5.056509   \n",
       "1       brain    ependymoma  12.643347  8.408933  7.893075  8.082575   \n",
       "2      normal        normal   8.523559  6.446557  6.110491  6.915887   \n",
       "3       brain  glioblastoma  12.180322  8.993027  8.008274  9.493953   \n",
       "4      normal        normal  11.196461  5.821402  5.456620  8.542700   \n",
       "\n",
       "   1255_g_at   1294_at   1316_at   1320_at  ...  AFFX-r2-Ec-bioD-3_at  \\\n",
       "0   2.785774  6.570793  6.135645  3.029876  ...             11.091654   \n",
       "1   4.091167  9.377509  6.116171  6.018412  ...             13.289336   \n",
       "2   2.375614  7.489329  4.136052  4.581049  ...             12.085742   \n",
       "3   4.454340  9.052279  6.331221  6.812413  ...             11.998082   \n",
       "4   3.245398  8.368442  6.583196  4.639983  ...             12.833154   \n",
       "\n",
       "   AFFX-r2-Ec-bioD-5_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "0             10.993969            12.576917            12.313470   \n",
       "1             12.742540            14.198576            13.909929   \n",
       "2             11.495483            13.630228            13.335569   \n",
       "3             11.357289            13.492403            13.329294   \n",
       "4             12.251775            14.016996            13.851353   \n",
       "\n",
       "   AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "0        2.599384        2.440502        1.982966         1.797714   \n",
       "1        5.403896        4.932116        3.942146         4.306064   \n",
       "2        8.594401        6.919923        7.472751         2.284780   \n",
       "3        5.252382        4.974248        4.039301         3.655452   \n",
       "4       10.713623        5.923269        8.745234         3.344703   \n",
       "\n",
       "   AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "0         1.964544         2.370085  \n",
       "1         4.841028         4.758554  \n",
       "2         2.585936         2.410203  \n",
       "3         4.483640         4.882925  \n",
       "4         3.553666         3.650059  \n",
       "\n",
       "[5 rows x 54677 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv(\"Dataset/brain+normal.csv\")\n",
    "df = shuffle(df, random_state=1).reset_index(drop=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>1431_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
       "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.556081</td>\n",
       "      <td>5.526080</td>\n",
       "      <td>3.650048</td>\n",
       "      <td>5.056509</td>\n",
       "      <td>2.785774</td>\n",
       "      <td>6.570793</td>\n",
       "      <td>6.135645</td>\n",
       "      <td>3.029876</td>\n",
       "      <td>2.366091</td>\n",
       "      <td>3.169992</td>\n",
       "      <td>...</td>\n",
       "      <td>11.091654</td>\n",
       "      <td>10.993969</td>\n",
       "      <td>12.576917</td>\n",
       "      <td>12.313470</td>\n",
       "      <td>2.599384</td>\n",
       "      <td>2.440502</td>\n",
       "      <td>1.982966</td>\n",
       "      <td>1.797714</td>\n",
       "      <td>1.964544</td>\n",
       "      <td>2.370085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.643347</td>\n",
       "      <td>8.408933</td>\n",
       "      <td>7.893075</td>\n",
       "      <td>8.082575</td>\n",
       "      <td>4.091167</td>\n",
       "      <td>9.377509</td>\n",
       "      <td>6.116171</td>\n",
       "      <td>6.018412</td>\n",
       "      <td>5.915260</td>\n",
       "      <td>6.227347</td>\n",
       "      <td>...</td>\n",
       "      <td>13.289336</td>\n",
       "      <td>12.742540</td>\n",
       "      <td>14.198576</td>\n",
       "      <td>13.909929</td>\n",
       "      <td>5.403896</td>\n",
       "      <td>4.932116</td>\n",
       "      <td>3.942146</td>\n",
       "      <td>4.306064</td>\n",
       "      <td>4.841028</td>\n",
       "      <td>4.758554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.523559</td>\n",
       "      <td>6.446557</td>\n",
       "      <td>6.110491</td>\n",
       "      <td>6.915887</td>\n",
       "      <td>2.375614</td>\n",
       "      <td>7.489329</td>\n",
       "      <td>4.136052</td>\n",
       "      <td>4.581049</td>\n",
       "      <td>10.589157</td>\n",
       "      <td>3.765427</td>\n",
       "      <td>...</td>\n",
       "      <td>12.085742</td>\n",
       "      <td>11.495483</td>\n",
       "      <td>13.630228</td>\n",
       "      <td>13.335569</td>\n",
       "      <td>8.594401</td>\n",
       "      <td>6.919923</td>\n",
       "      <td>7.472751</td>\n",
       "      <td>2.284780</td>\n",
       "      <td>2.585936</td>\n",
       "      <td>2.410203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.180322</td>\n",
       "      <td>8.993027</td>\n",
       "      <td>8.008274</td>\n",
       "      <td>9.493953</td>\n",
       "      <td>4.454340</td>\n",
       "      <td>9.052279</td>\n",
       "      <td>6.331221</td>\n",
       "      <td>6.812413</td>\n",
       "      <td>7.634342</td>\n",
       "      <td>5.272973</td>\n",
       "      <td>...</td>\n",
       "      <td>11.998082</td>\n",
       "      <td>11.357289</td>\n",
       "      <td>13.492403</td>\n",
       "      <td>13.329294</td>\n",
       "      <td>5.252382</td>\n",
       "      <td>4.974248</td>\n",
       "      <td>4.039301</td>\n",
       "      <td>3.655452</td>\n",
       "      <td>4.483640</td>\n",
       "      <td>4.882925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.196461</td>\n",
       "      <td>5.821402</td>\n",
       "      <td>5.456620</td>\n",
       "      <td>8.542700</td>\n",
       "      <td>3.245398</td>\n",
       "      <td>8.368442</td>\n",
       "      <td>6.583196</td>\n",
       "      <td>4.639983</td>\n",
       "      <td>6.937847</td>\n",
       "      <td>3.650488</td>\n",
       "      <td>...</td>\n",
       "      <td>12.833154</td>\n",
       "      <td>12.251775</td>\n",
       "      <td>14.016996</td>\n",
       "      <td>13.851353</td>\n",
       "      <td>10.713623</td>\n",
       "      <td>5.923269</td>\n",
       "      <td>8.745234</td>\n",
       "      <td>3.344703</td>\n",
       "      <td>3.553666</td>\n",
       "      <td>3.650059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1007_s_at   1053_at    117_at    121_at  1255_g_at   1294_at   1316_at  \\\n",
       "0   9.556081  5.526080  3.650048  5.056509   2.785774  6.570793  6.135645   \n",
       "1  12.643347  8.408933  7.893075  8.082575   4.091167  9.377509  6.116171   \n",
       "2   8.523559  6.446557  6.110491  6.915887   2.375614  7.489329  4.136052   \n",
       "3  12.180322  8.993027  8.008274  9.493953   4.454340  9.052279  6.331221   \n",
       "4  11.196461  5.821402  5.456620  8.542700   3.245398  8.368442  6.583196   \n",
       "\n",
       "    1320_at  1405_i_at   1431_at  ...  AFFX-r2-Ec-bioD-3_at  \\\n",
       "0  3.029876   2.366091  3.169992  ...             11.091654   \n",
       "1  6.018412   5.915260  6.227347  ...             13.289336   \n",
       "2  4.581049  10.589157  3.765427  ...             12.085742   \n",
       "3  6.812413   7.634342  5.272973  ...             11.998082   \n",
       "4  4.639983   6.937847  3.650488  ...             12.833154   \n",
       "\n",
       "   AFFX-r2-Ec-bioD-5_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "0             10.993969            12.576917            12.313470   \n",
       "1             12.742540            14.198576            13.909929   \n",
       "2             11.495483            13.630228            13.335569   \n",
       "3             11.357289            13.492403            13.329294   \n",
       "4             12.251775            14.016996            13.851353   \n",
       "\n",
       "   AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "0        2.599384        2.440502        1.982966         1.797714   \n",
       "1        5.403896        4.932116        3.942146         4.306064   \n",
       "2        8.594401        6.919923        7.472751         2.284780   \n",
       "3        5.252382        4.974248        4.039301         3.655452   \n",
       "4       10.713623        5.923269        8.745234         3.344703   \n",
       "\n",
       "   AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "0         1.964544         2.370085  \n",
       "1         4.841028         4.758554  \n",
       "2         2.585936         2.410203  \n",
       "3         4.483640         4.882925  \n",
       "4         3.553666         3.650059  \n",
       "\n",
       "[5 rows x 54675 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(278, 54675)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: cancer_type, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(278,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    # Drop type Columns\n",
    "    if \"type\" in df.columns:\n",
    "        df = df.drop(columns=\"type\")\n",
    "\n",
    "    # Convert label to binary type:\n",
    "    if 'cancer_type' in df.columns and not df['cancer_type'].isin([0, 1]).all():\n",
    "        df['cancer_type'] = df['cancer_type'].map({'brain': 1, 'normal': 0})\n",
    "    \n",
    "    # Get X,y\n",
    "    target = 'cancer_type'\n",
    "    X = df.drop(columns=target)\n",
    "    y = df[target]\n",
    "    \n",
    "    return X,y \n",
    "\n",
    "X,y = preprocessing(df)\n",
    "display(X.head(5), X.shape)\n",
    "display(y.head(5), y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features: 54675\n",
      "Selected features: Index(['1007_s_at', '1053_at', '117_at', '121_at', '1255_g_at', '1294_at',\n",
      "       '1316_at', '1320_at', '1405_i_at', '1431_at',\n",
      "       ...\n",
      "       'AFFX-r2-Ec-bioD-3_at', 'AFFX-r2-Ec-bioD-5_at', 'AFFX-r2-P1-cre-3_at',\n",
      "       'AFFX-r2-P1-cre-5_at', 'AFFX-ThrX-3_at', 'AFFX-ThrX-5_at',\n",
      "       'AFFX-ThrX-M_at', 'AFFX-TrpnX-3_at', 'AFFX-TrpnX-5_at',\n",
      "       'AFFX-TrpnX-M_at'],\n",
      "      dtype='object', length=54675)\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Variance Threshold (Assume Features with higher variance => better)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "\n",
    "threshold = 0.01\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "X_var = selector.fit_transform(X)\n",
    "\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "X_var = X.columns[selected_feature_indices]\n",
    "print(f\"Number of selected features: {len(X_var)}\")\n",
    "print(f\"Selected features: {X_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['1552439_s_at', '1552518_s_at', '1553122_s_at', '1553268_at',\n",
      "       '1553689_s_at', '1553720_a_at', '1553959_a_at', '1553998_at',\n",
      "       '1554080_at', '1554153_a_at',\n",
      "       ...\n",
      "       '244403_at', '244703_x_at', '244708_at', '244740_at', '244819_x_at',\n",
      "       '37892_at', '39248_at', '39966_at', '48808_at', '65493_at'],\n",
      "      dtype='object', length=500)\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Unvariate feature selection method (Based on their relation with output)\n",
    "# ANOVA f-statistics\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 1000 features based on ANOVA F-statistic\n",
    "k_best_selector = SelectKBest(score_func=f_classif, k=500)\n",
    "X_anova = k_best_selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "X_anova = X.columns[k_best_selector.get_support()]\n",
    "\n",
    "print(f\"Selected features: {X_anova}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['1552468_a_at', '1552503_at', '1552515_at', '1552564_at',\n",
      "       '1552713_a_at', '1552852_a_at', '1552877_s_at', '1552960_at',\n",
      "       '1552979_at', '1553207_at',\n",
      "       ...\n",
      "       '37020_at', '39966_at', 'AFFX-DapX-3_at', 'AFFX-DapX-5_at',\n",
      "       'AFFX-DapX-M_at', 'AFFX-LysX-3_at', 'AFFX-PheX-M_at',\n",
      "       'AFFX-r2-Bs-dap-3_at', 'AFFX-r2-Bs-phe-3_at', 'AFFX-ThrX-M_at'],\n",
      "      dtype='object', length=500)\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Mutual Information methnod (Non-linear relationship between predictors and targets)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Select top 100 features based on Mutual Information\n",
    "mutual_info_selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "X_mut = mutual_info_selector.fit_transform(X, y) \n",
    "\n",
    "# Get selected feature names\n",
    "X_mut = X.columns[mutual_info_selector.get_support()]\n",
    "\n",
    "print(f\"Selected features: {X_mut}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3  \n",
    "\n",
    "# Convert numpy arrays to flattened lists for compatibility\n",
    "variance_features = set(X_var.ravel())\n",
    "anova_features = set(X_anova.ravel())\n",
    "mutual_info_features = set(X_mut.ravel())\n",
    "\n",
    "# Create a Venn diagram to visualize the overlap\n",
    "plt.figure(figsize=(8, 6))\n",
    "venn_diagram = venn3(\n",
    "    subsets=(\n",
    "        len(variance_features - anova_features - mutual_info_features),  # Only Variance\n",
    "        len(anova_features - variance_features - mutual_info_features),  # Only ANOVA\n",
    "        len(variance_features & anova_features - mutual_info_features),  # Variance & ANOVA\n",
    "        len(mutual_info_features - variance_features - anova_features),  # Only Mutual Info\n",
    "        len(variance_features & mutual_info_features - anova_features),  # Variance & Mutual Info\n",
    "        len(anova_features & mutual_info_features - variance_features),  # ANOVA & Mutual Info\n",
    "        len(variance_features & anova_features & mutual_info_features),  # All 3 Methods\n",
    "    ),\n",
    "    set_labels=(\"Variance Threshold\", \"ANOVA\", \"Mutual Information\"),\n",
    ")\n",
    "plt.title(\"Feature Selection Overlap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>213710_s_at</th>\n",
       "      <th>205137_x_at</th>\n",
       "      <th>232380_at</th>\n",
       "      <th>208119_s_at</th>\n",
       "      <th>211545_at</th>\n",
       "      <th>228332_s_at</th>\n",
       "      <th>221198_at</th>\n",
       "      <th>243779_at</th>\n",
       "      <th>211121_s_at</th>\n",
       "      <th>1556824_at</th>\n",
       "      <th>...</th>\n",
       "      <th>229683_s_at</th>\n",
       "      <th>215945_s_at</th>\n",
       "      <th>224563_at</th>\n",
       "      <th>244518_at</th>\n",
       "      <th>235388_at</th>\n",
       "      <th>1558937_s_at</th>\n",
       "      <th>230651_at</th>\n",
       "      <th>201412_at</th>\n",
       "      <th>1566903_at</th>\n",
       "      <th>220133_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.287570</td>\n",
       "      <td>4.529808</td>\n",
       "      <td>1.982755</td>\n",
       "      <td>5.890185</td>\n",
       "      <td>2.588576</td>\n",
       "      <td>10.623664</td>\n",
       "      <td>2.993924</td>\n",
       "      <td>5.544368</td>\n",
       "      <td>4.306138</td>\n",
       "      <td>3.747814</td>\n",
       "      <td>...</td>\n",
       "      <td>2.477682</td>\n",
       "      <td>7.336640</td>\n",
       "      <td>6.375150</td>\n",
       "      <td>1.823107</td>\n",
       "      <td>4.825138</td>\n",
       "      <td>3.833613</td>\n",
       "      <td>6.172199</td>\n",
       "      <td>7.842730</td>\n",
       "      <td>4.706900</td>\n",
       "      <td>2.058751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.321853</td>\n",
       "      <td>6.688427</td>\n",
       "      <td>3.977298</td>\n",
       "      <td>6.888862</td>\n",
       "      <td>6.560454</td>\n",
       "      <td>10.982021</td>\n",
       "      <td>7.786743</td>\n",
       "      <td>5.615331</td>\n",
       "      <td>8.467095</td>\n",
       "      <td>7.000674</td>\n",
       "      <td>...</td>\n",
       "      <td>6.534304</td>\n",
       "      <td>9.772156</td>\n",
       "      <td>6.750810</td>\n",
       "      <td>5.342392</td>\n",
       "      <td>5.794413</td>\n",
       "      <td>6.860871</td>\n",
       "      <td>7.160835</td>\n",
       "      <td>11.429598</td>\n",
       "      <td>6.311937</td>\n",
       "      <td>5.578781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.718628</td>\n",
       "      <td>5.442672</td>\n",
       "      <td>2.448124</td>\n",
       "      <td>6.355641</td>\n",
       "      <td>3.163719</td>\n",
       "      <td>10.099239</td>\n",
       "      <td>4.214922</td>\n",
       "      <td>3.162561</td>\n",
       "      <td>6.343606</td>\n",
       "      <td>4.786545</td>\n",
       "      <td>...</td>\n",
       "      <td>3.091369</td>\n",
       "      <td>6.294298</td>\n",
       "      <td>5.911147</td>\n",
       "      <td>2.445018</td>\n",
       "      <td>6.950301</td>\n",
       "      <td>6.482961</td>\n",
       "      <td>5.577999</td>\n",
       "      <td>9.333484</td>\n",
       "      <td>4.262755</td>\n",
       "      <td>3.609925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.495093</td>\n",
       "      <td>7.215526</td>\n",
       "      <td>4.103611</td>\n",
       "      <td>7.654564</td>\n",
       "      <td>5.882514</td>\n",
       "      <td>11.355758</td>\n",
       "      <td>5.046568</td>\n",
       "      <td>5.292004</td>\n",
       "      <td>9.161231</td>\n",
       "      <td>7.208103</td>\n",
       "      <td>...</td>\n",
       "      <td>5.956374</td>\n",
       "      <td>8.143264</td>\n",
       "      <td>7.800196</td>\n",
       "      <td>5.362307</td>\n",
       "      <td>5.862531</td>\n",
       "      <td>7.176496</td>\n",
       "      <td>5.761513</td>\n",
       "      <td>11.733236</td>\n",
       "      <td>7.080405</td>\n",
       "      <td>5.493645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.469755</td>\n",
       "      <td>10.205048</td>\n",
       "      <td>3.282063</td>\n",
       "      <td>6.304804</td>\n",
       "      <td>4.299444</td>\n",
       "      <td>11.464096</td>\n",
       "      <td>4.889009</td>\n",
       "      <td>3.680064</td>\n",
       "      <td>6.650419</td>\n",
       "      <td>6.529323</td>\n",
       "      <td>...</td>\n",
       "      <td>4.776193</td>\n",
       "      <td>7.678931</td>\n",
       "      <td>6.758020</td>\n",
       "      <td>3.722017</td>\n",
       "      <td>4.338000</td>\n",
       "      <td>4.241653</td>\n",
       "      <td>6.811022</td>\n",
       "      <td>11.725830</td>\n",
       "      <td>5.980042</td>\n",
       "      <td>3.769850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   213710_s_at  205137_x_at  232380_at  208119_s_at  211545_at  228332_s_at  \\\n",
       "0     7.287570     4.529808   1.982755     5.890185   2.588576    10.623664   \n",
       "1     8.321853     6.688427   3.977298     6.888862   6.560454    10.982021   \n",
       "2     6.718628     5.442672   2.448124     6.355641   3.163719    10.099239   \n",
       "3     8.495093     7.215526   4.103611     7.654564   5.882514    11.355758   \n",
       "4     8.469755    10.205048   3.282063     6.304804   4.299444    11.464096   \n",
       "\n",
       "   221198_at  243779_at  211121_s_at  1556824_at  ...  229683_s_at  \\\n",
       "0   2.993924   5.544368     4.306138    3.747814  ...     2.477682   \n",
       "1   7.786743   5.615331     8.467095    7.000674  ...     6.534304   \n",
       "2   4.214922   3.162561     6.343606    4.786545  ...     3.091369   \n",
       "3   5.046568   5.292004     9.161231    7.208103  ...     5.956374   \n",
       "4   4.889009   3.680064     6.650419    6.529323  ...     4.776193   \n",
       "\n",
       "   215945_s_at  224563_at  244518_at  235388_at  1558937_s_at  230651_at  \\\n",
       "0     7.336640   6.375150   1.823107   4.825138      3.833613   6.172199   \n",
       "1     9.772156   6.750810   5.342392   5.794413      6.860871   7.160835   \n",
       "2     6.294298   5.911147   2.445018   6.950301      6.482961   5.577999   \n",
       "3     8.143264   7.800196   5.362307   5.862531      7.176496   5.761513   \n",
       "4     7.678931   6.758020   3.722017   4.338000      4.241653   6.811022   \n",
       "\n",
       "   201412_at  1566903_at  220133_at  \n",
       "0   7.842730    4.706900   2.058751  \n",
       "1  11.429598    6.311937   5.578781  \n",
       "2   9.333484    4.262755   3.609925  \n",
       "3  11.733236    7.080405   5.493645  \n",
       "4  11.725830    5.980042   3.769850  \n",
       "\n",
       "[5 rows x 54675 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Perform feature_selection (ANOVA, Mutual Information, Reduce Overlap)\n",
    "def feature_selection(X,y):\n",
    "    # Perform ANOVA\n",
    "    k_best_selector = SelectKBest(score_func=f_classif, k=500)\n",
    "    X_anova = k_best_selector.fit_transform(X,y)\n",
    "    X_anova = X.columns[k_best_selector.get_support()]\n",
    "    \n",
    "    # Perform Mutual Information\n",
    "    mutual_info_selector = SelectKBest(score_func=mutual_info_classif, k=500)\n",
    "    X_mut = mutual_info_selector.fit_transform(X,y)\n",
    "    X_mut = X.columns[mutual_info_selector.get_support()]\n",
    "    \n",
    "    # # Reduce overlap\n",
    "    # X_reduce = set(X_anova).difference(set(X_mut))\n",
    "    # X_reduce = X[list(X_reduce)]\n",
    "    \n",
    "    # X_reduce = X_anova.intersection(X_mut)  # Features in both ANOVA and Mutual Information\n",
    "    # X_reduce = X[list(X_reduce)]  # Subset of data with overlapping features\n",
    "    \n",
    "    combined_features = set(X_anova).union(set(X_mut)).union(set(X_var))\n",
    "    X_reduce = X[list(combined_features)]  # Subset the data to include combined features\n",
    "    \n",
    "    return X_reduce\n",
    "\n",
    "X = feature_selection(X,y)\n",
    "display(X.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Results:\n",
      "  Accuracy: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "  ROC AUC: 1.0000\n",
      "------------------------------\n",
      "Fold 2 Results:\n",
      "  Accuracy: 0.9821\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9825\n",
      "  ROC AUC: 1.0000\n",
      "------------------------------\n",
      "Fold 3 Results:\n",
      "  Accuracy: 0.9643\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 0.9655\n",
      "  ROC AUC: 0.9898\n",
      "------------------------------\n",
      "Fold 4 Results:\n",
      "  Accuracy: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "  ROC AUC: 1.0000\n",
      "------------------------------\n",
      "Fold 5 Results:\n",
      "  Accuracy: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "  ROC AUC: 1.0000\n",
      "------------------------------\n",
      "\n",
      "Cross-Validation Summary:\n",
      "Mean Accuracy: 0.9893\n",
      "Mean Recall: 1.0000\n",
      "Mean F1 Score: 0.9896\n",
      "Mean ROC AUC: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Define Training Pipeline with Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Adjust n_estimators as needed\n",
    "])\n",
    "\n",
    "# Define Stratified K-Fold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "fold = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Split the data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    # Append scores\n",
    "    accuracy_scores.append(accuracy)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    # Print results for the current fold\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "# Final summary of metrics\n",
    "print(\"\\nCross-Validation Summary:\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Mean ROC AUC: {np.mean(roc_auc_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "Mean Accuracy: 0.99 ± 0.01\n",
      "Mean Recall: 1.00 ± 0.00\n",
      "Mean F1 Score: 0.99 ± 0.01\n",
      "Mean ROC-AUC: 1.00 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "# Final Train accuracy\n",
    "\n",
    "# Print average metrics across folds\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracy_scores):.2f} ± {np.std(accuracy_scores):.2f}\")\n",
    "print(f\"Mean Recall: {np.mean(recall_scores):.2f} ± {np.std(recall_scores):.2f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.2f} ± {np.std(f1_scores):.2f}\")\n",
    "print(f\"Mean ROC-AUC: {np.mean(roc_auc_scores):.2f} ± {np.std(roc_auc_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>213710_s_at</th>\n",
       "      <th>205137_x_at</th>\n",
       "      <th>232380_at</th>\n",
       "      <th>208119_s_at</th>\n",
       "      <th>211545_at</th>\n",
       "      <th>228332_s_at</th>\n",
       "      <th>221198_at</th>\n",
       "      <th>243779_at</th>\n",
       "      <th>211121_s_at</th>\n",
       "      <th>1556824_at</th>\n",
       "      <th>...</th>\n",
       "      <th>229683_s_at</th>\n",
       "      <th>215945_s_at</th>\n",
       "      <th>224563_at</th>\n",
       "      <th>244518_at</th>\n",
       "      <th>235388_at</th>\n",
       "      <th>1558937_s_at</th>\n",
       "      <th>230651_at</th>\n",
       "      <th>201412_at</th>\n",
       "      <th>1566903_at</th>\n",
       "      <th>220133_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.162039</td>\n",
       "      <td>7.308749</td>\n",
       "      <td>4.059560</td>\n",
       "      <td>7.888039</td>\n",
       "      <td>5.557629</td>\n",
       "      <td>11.630465</td>\n",
       "      <td>6.703813</td>\n",
       "      <td>4.619464</td>\n",
       "      <td>7.805253</td>\n",
       "      <td>7.346009</td>\n",
       "      <td>...</td>\n",
       "      <td>5.690123</td>\n",
       "      <td>8.678514</td>\n",
       "      <td>6.487800</td>\n",
       "      <td>5.483426</td>\n",
       "      <td>5.121286</td>\n",
       "      <td>7.292486</td>\n",
       "      <td>6.306852</td>\n",
       "      <td>10.729298</td>\n",
       "      <td>6.925063</td>\n",
       "      <td>6.076351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.293118</td>\n",
       "      <td>6.055362</td>\n",
       "      <td>4.384060</td>\n",
       "      <td>6.989815</td>\n",
       "      <td>5.807919</td>\n",
       "      <td>11.050265</td>\n",
       "      <td>6.990988</td>\n",
       "      <td>4.905639</td>\n",
       "      <td>8.098224</td>\n",
       "      <td>6.717531</td>\n",
       "      <td>...</td>\n",
       "      <td>5.178632</td>\n",
       "      <td>9.407581</td>\n",
       "      <td>10.553699</td>\n",
       "      <td>4.993173</td>\n",
       "      <td>7.765337</td>\n",
       "      <td>6.141427</td>\n",
       "      <td>6.882173</td>\n",
       "      <td>11.571720</td>\n",
       "      <td>6.482593</td>\n",
       "      <td>4.789440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.361826</td>\n",
       "      <td>6.605863</td>\n",
       "      <td>3.738974</td>\n",
       "      <td>7.376319</td>\n",
       "      <td>5.312107</td>\n",
       "      <td>11.830015</td>\n",
       "      <td>6.833965</td>\n",
       "      <td>5.849274</td>\n",
       "      <td>6.945981</td>\n",
       "      <td>7.191664</td>\n",
       "      <td>...</td>\n",
       "      <td>5.981942</td>\n",
       "      <td>10.047993</td>\n",
       "      <td>6.828490</td>\n",
       "      <td>4.786643</td>\n",
       "      <td>6.484462</td>\n",
       "      <td>5.682304</td>\n",
       "      <td>6.322076</td>\n",
       "      <td>9.673931</td>\n",
       "      <td>6.191607</td>\n",
       "      <td>5.597522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.896389</td>\n",
       "      <td>7.799217</td>\n",
       "      <td>3.982728</td>\n",
       "      <td>8.007835</td>\n",
       "      <td>5.589116</td>\n",
       "      <td>11.501834</td>\n",
       "      <td>6.370530</td>\n",
       "      <td>4.670558</td>\n",
       "      <td>7.971494</td>\n",
       "      <td>6.742999</td>\n",
       "      <td>...</td>\n",
       "      <td>5.251052</td>\n",
       "      <td>10.037643</td>\n",
       "      <td>9.182483</td>\n",
       "      <td>4.835485</td>\n",
       "      <td>7.253448</td>\n",
       "      <td>7.308030</td>\n",
       "      <td>7.011981</td>\n",
       "      <td>10.951755</td>\n",
       "      <td>6.215221</td>\n",
       "      <td>5.520285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.306310</td>\n",
       "      <td>6.039583</td>\n",
       "      <td>4.255391</td>\n",
       "      <td>8.517859</td>\n",
       "      <td>5.673243</td>\n",
       "      <td>11.034767</td>\n",
       "      <td>6.552751</td>\n",
       "      <td>5.795334</td>\n",
       "      <td>7.112075</td>\n",
       "      <td>7.256868</td>\n",
       "      <td>...</td>\n",
       "      <td>5.359473</td>\n",
       "      <td>9.700631</td>\n",
       "      <td>6.433242</td>\n",
       "      <td>5.664275</td>\n",
       "      <td>5.454613</td>\n",
       "      <td>6.925461</td>\n",
       "      <td>6.283540</td>\n",
       "      <td>10.289821</td>\n",
       "      <td>7.095037</td>\n",
       "      <td>5.878101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>8.348692</td>\n",
       "      <td>7.909219</td>\n",
       "      <td>4.561985</td>\n",
       "      <td>7.112701</td>\n",
       "      <td>6.007605</td>\n",
       "      <td>11.989649</td>\n",
       "      <td>6.127715</td>\n",
       "      <td>4.476899</td>\n",
       "      <td>6.977326</td>\n",
       "      <td>7.991831</td>\n",
       "      <td>...</td>\n",
       "      <td>6.129060</td>\n",
       "      <td>8.530331</td>\n",
       "      <td>7.746663</td>\n",
       "      <td>5.526233</td>\n",
       "      <td>4.840287</td>\n",
       "      <td>5.610489</td>\n",
       "      <td>7.628834</td>\n",
       "      <td>12.288878</td>\n",
       "      <td>7.243273</td>\n",
       "      <td>9.103671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9.717776</td>\n",
       "      <td>7.563727</td>\n",
       "      <td>4.917422</td>\n",
       "      <td>7.726204</td>\n",
       "      <td>5.600448</td>\n",
       "      <td>11.396366</td>\n",
       "      <td>6.266361</td>\n",
       "      <td>4.694214</td>\n",
       "      <td>6.614438</td>\n",
       "      <td>8.371601</td>\n",
       "      <td>...</td>\n",
       "      <td>6.141777</td>\n",
       "      <td>8.539390</td>\n",
       "      <td>7.106021</td>\n",
       "      <td>5.607257</td>\n",
       "      <td>5.303470</td>\n",
       "      <td>5.650333</td>\n",
       "      <td>7.169580</td>\n",
       "      <td>10.264687</td>\n",
       "      <td>7.216512</td>\n",
       "      <td>8.607215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.248133</td>\n",
       "      <td>4.844204</td>\n",
       "      <td>2.969710</td>\n",
       "      <td>6.857049</td>\n",
       "      <td>3.974126</td>\n",
       "      <td>7.429815</td>\n",
       "      <td>3.520831</td>\n",
       "      <td>3.205050</td>\n",
       "      <td>5.153114</td>\n",
       "      <td>4.555247</td>\n",
       "      <td>...</td>\n",
       "      <td>3.530796</td>\n",
       "      <td>5.004804</td>\n",
       "      <td>3.678768</td>\n",
       "      <td>3.809075</td>\n",
       "      <td>2.237177</td>\n",
       "      <td>2.604035</td>\n",
       "      <td>5.788014</td>\n",
       "      <td>4.579917</td>\n",
       "      <td>4.845387</td>\n",
       "      <td>3.383381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5.099312</td>\n",
       "      <td>5.369723</td>\n",
       "      <td>2.583310</td>\n",
       "      <td>7.408019</td>\n",
       "      <td>4.260803</td>\n",
       "      <td>3.997443</td>\n",
       "      <td>3.114914</td>\n",
       "      <td>2.959267</td>\n",
       "      <td>5.043678</td>\n",
       "      <td>4.640307</td>\n",
       "      <td>...</td>\n",
       "      <td>3.492059</td>\n",
       "      <td>4.352712</td>\n",
       "      <td>3.864623</td>\n",
       "      <td>3.776005</td>\n",
       "      <td>2.298872</td>\n",
       "      <td>3.620220</td>\n",
       "      <td>4.178832</td>\n",
       "      <td>4.286105</td>\n",
       "      <td>5.105831</td>\n",
       "      <td>3.364206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4.777936</td>\n",
       "      <td>4.561181</td>\n",
       "      <td>2.346855</td>\n",
       "      <td>5.923113</td>\n",
       "      <td>3.565039</td>\n",
       "      <td>7.651237</td>\n",
       "      <td>3.023406</td>\n",
       "      <td>2.609594</td>\n",
       "      <td>4.705826</td>\n",
       "      <td>3.912701</td>\n",
       "      <td>...</td>\n",
       "      <td>3.159059</td>\n",
       "      <td>5.161740</td>\n",
       "      <td>3.284770</td>\n",
       "      <td>4.639369</td>\n",
       "      <td>2.248707</td>\n",
       "      <td>3.094867</td>\n",
       "      <td>3.898465</td>\n",
       "      <td>5.519874</td>\n",
       "      <td>5.300147</td>\n",
       "      <td>3.011259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 54675 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    213710_s_at  205137_x_at  232380_at  208119_s_at  211545_at  228332_s_at  \\\n",
       "0      9.162039     7.308749   4.059560     7.888039   5.557629    11.630465   \n",
       "1      8.293118     6.055362   4.384060     6.989815   5.807919    11.050265   \n",
       "2      9.361826     6.605863   3.738974     7.376319   5.312107    11.830015   \n",
       "3      8.896389     7.799217   3.982728     8.007835   5.589116    11.501834   \n",
       "4      9.306310     6.039583   4.255391     8.517859   5.673243    11.034767   \n",
       "..          ...          ...        ...          ...        ...          ...   \n",
       "62     8.348692     7.909219   4.561985     7.112701   6.007605    11.989649   \n",
       "63     9.717776     7.563727   4.917422     7.726204   5.600448    11.396366   \n",
       "64     5.248133     4.844204   2.969710     6.857049   3.974126     7.429815   \n",
       "65     5.099312     5.369723   2.583310     7.408019   4.260803     3.997443   \n",
       "66     4.777936     4.561181   2.346855     5.923113   3.565039     7.651237   \n",
       "\n",
       "    221198_at  243779_at  211121_s_at  1556824_at  ...  229683_s_at  \\\n",
       "0    6.703813   4.619464     7.805253    7.346009  ...     5.690123   \n",
       "1    6.990988   4.905639     8.098224    6.717531  ...     5.178632   \n",
       "2    6.833965   5.849274     6.945981    7.191664  ...     5.981942   \n",
       "3    6.370530   4.670558     7.971494    6.742999  ...     5.251052   \n",
       "4    6.552751   5.795334     7.112075    7.256868  ...     5.359473   \n",
       "..        ...        ...          ...         ...  ...          ...   \n",
       "62   6.127715   4.476899     6.977326    7.991831  ...     6.129060   \n",
       "63   6.266361   4.694214     6.614438    8.371601  ...     6.141777   \n",
       "64   3.520831   3.205050     5.153114    4.555247  ...     3.530796   \n",
       "65   3.114914   2.959267     5.043678    4.640307  ...     3.492059   \n",
       "66   3.023406   2.609594     4.705826    3.912701  ...     3.159059   \n",
       "\n",
       "    215945_s_at  224563_at  244518_at  235388_at  1558937_s_at  230651_at  \\\n",
       "0      8.678514   6.487800   5.483426   5.121286      7.292486   6.306852   \n",
       "1      9.407581  10.553699   4.993173   7.765337      6.141427   6.882173   \n",
       "2     10.047993   6.828490   4.786643   6.484462      5.682304   6.322076   \n",
       "3     10.037643   9.182483   4.835485   7.253448      7.308030   7.011981   \n",
       "4      9.700631   6.433242   5.664275   5.454613      6.925461   6.283540   \n",
       "..          ...        ...        ...        ...           ...        ...   \n",
       "62     8.530331   7.746663   5.526233   4.840287      5.610489   7.628834   \n",
       "63     8.539390   7.106021   5.607257   5.303470      5.650333   7.169580   \n",
       "64     5.004804   3.678768   3.809075   2.237177      2.604035   5.788014   \n",
       "65     4.352712   3.864623   3.776005   2.298872      3.620220   4.178832   \n",
       "66     5.161740   3.284770   4.639369   2.248707      3.094867   3.898465   \n",
       "\n",
       "    201412_at  1566903_at  220133_at  \n",
       "0   10.729298    6.925063   6.076351  \n",
       "1   11.571720    6.482593   4.789440  \n",
       "2    9.673931    6.191607   5.597522  \n",
       "3   10.951755    6.215221   5.520285  \n",
       "4   10.289821    7.095037   5.878101  \n",
       "..        ...         ...        ...  \n",
       "62  12.288878    7.243273   9.103671  \n",
       "63  10.264687    7.216512   8.607215  \n",
       "64   4.579917    4.845387   3.383381  \n",
       "65   4.286105    5.105831   3.364206  \n",
       "66   5.519874    5.300147   3.011259  \n",
       "\n",
       "[67 rows x 54675 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     0\n",
       "3     1\n",
       "4     0\n",
       "     ..\n",
       "62    0\n",
       "63    0\n",
       "64    0\n",
       "65    0\n",
       "66    0\n",
       "Name: cancer_type, Length: 67, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load and shuffle the dataset\n",
    "test_breast = pd.read_csv(\"Dataset/brain_test_data.csv\")\n",
    "X_test, y_test = preprocessing(test_breast)\n",
    "X_test = X_test[X.columns]\n",
    "display(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation on Test Dataset:\n",
      "Accuracy: 0.99\n",
      "Recall: 1.00\n",
      "F1 Score: 0.97\n",
      "ROC-AUC: 1.00\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        52\n",
      "           1       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.99        67\n",
      "   macro avg       0.97      0.99      0.98        67\n",
      "weighted avg       0.99      0.99      0.99        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict using the final trained model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFinal Model Evaluation on Test Dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction report saved to Dataset/test_set_prediction_report.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the report dataframe\n",
    "report_df = pd.DataFrame({\n",
    "    'True Label': y_test,\n",
    "    'Predicted Label': y_pred,\n",
    "    'Probability (Brain)': y_pred_prob\n",
    "})\n",
    "\n",
    "# Save the report to the Dataset directory\n",
    "output_path = \"Dataset/test_set_prediction_report.csv\"\n",
    "report_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Prediction report saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0: Predicted Probability (Cancer - Class 1): 0.8900\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.186898\n",
      "40879  218899_s_at      0.172857\n",
      "9660     239082_at      0.168412\n",
      "49908  223313_s_at      0.131539\n",
      "13313    221909_at      0.129196\n",
      "730      205103_at      0.127211\n",
      "45685    223642_at      0.105423\n",
      "35407    227933_at      0.102371\n",
      "15398    212909_at      0.099410\n",
      "28738    235238_at      0.098321\n",
      "\n",
      "Sample 1: Predicted Probability (Cancer - Class 1): 0.9800\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.206487\n",
      "730      205103_at      0.187422\n",
      "40879  218899_s_at      0.186474\n",
      "9660     239082_at      0.172854\n",
      "35407    227933_at      0.167568\n",
      "1563     244403_at      0.161089\n",
      "49908  223313_s_at      0.148444\n",
      "13313    221909_at      0.141262\n",
      "28738    235238_at      0.129899\n",
      "45685    223642_at      0.120553\n",
      "\n",
      "Sample 3: Predicted Probability (Cancer - Class 1): 0.9700\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.197288\n",
      "9660     239082_at      0.178999\n",
      "1563     244403_at      0.176223\n",
      "730      205103_at      0.168418\n",
      "49908  223313_s_at      0.165294\n",
      "40879  218899_s_at      0.148944\n",
      "13313    221909_at      0.141595\n",
      "35407    227933_at      0.133926\n",
      "45685    223642_at      0.121969\n",
      "28738    235238_at      0.120360\n",
      "\n",
      "Sample 4: Predicted Probability (Cancer - Class 1): 0.7600\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.210512\n",
      "730      205103_at      0.197737\n",
      "11859    219619_at      0.187255\n",
      "40879  218899_s_at      0.186914\n",
      "9660     239082_at      0.162911\n",
      "35407    227933_at      0.160141\n",
      "49908  223313_s_at      0.154029\n",
      "13313    221909_at      0.145615\n",
      "1563     244403_at      0.144478\n",
      "45685    223642_at      0.126844\n",
      "\n",
      "Sample 5: Predicted Probability (Cancer - Class 1): 0.9800\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "730      205103_at      0.182040\n",
      "4164     212233_at      0.181973\n",
      "40879  218899_s_at      0.154392\n",
      "49908  223313_s_at      0.146484\n",
      "1563     244403_at      0.145119\n",
      "13313    221909_at      0.140054\n",
      "9660     239082_at      0.136937\n",
      "35407    227933_at      0.118687\n",
      "37619    230272_at      0.101902\n",
      "28738    235238_at      0.099883\n",
      "\n",
      "Sample 6: Predicted Probability (Cancer - Class 1): 0.9500\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.199842\n",
      "730      205103_at      0.173476\n",
      "40879  218899_s_at      0.170154\n",
      "9660     239082_at      0.159938\n",
      "49908  223313_s_at      0.155057\n",
      "1563     244403_at      0.150244\n",
      "13313    221909_at      0.142117\n",
      "35407    227933_at      0.138196\n",
      "28738    235238_at      0.122243\n",
      "45685    223642_at      0.114304\n",
      "\n",
      "Sample 7: Predicted Probability (Cancer - Class 1): 0.8600\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.172145\n",
      "49908  223313_s_at      0.155721\n",
      "40879  218899_s_at      0.123035\n",
      "30588  201387_s_at      0.103259\n",
      "35407    227933_at      0.102435\n",
      "41652    209014_at      0.097417\n",
      "13313    221909_at      0.082418\n",
      "15965  212639_x_at      0.081900\n",
      "730      205103_at      0.078071\n",
      "13619    201462_at      0.071167\n",
      "\n",
      "Sample 8: Predicted Probability (Cancer - Class 1): 1.0000\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "730      205103_at      0.205896\n",
      "4164     212233_at      0.199918\n",
      "40879  218899_s_at      0.182543\n",
      "49908  223313_s_at      0.175146\n",
      "35407    227933_at      0.167012\n",
      "13313    221909_at      0.164551\n",
      "1563     244403_at      0.161855\n",
      "9660     239082_at      0.152277\n",
      "28738    235238_at      0.146240\n",
      "37619    230272_at      0.109990\n",
      "\n",
      "Sample 9: Predicted Probability (Cancer - Class 1): 0.9600\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "730      205103_at      0.185798\n",
      "4164     212233_at      0.174417\n",
      "40879  218899_s_at      0.163975\n",
      "49908  223313_s_at      0.155793\n",
      "9660     239082_at      0.146927\n",
      "1563     244403_at      0.139607\n",
      "35407    227933_at      0.122293\n",
      "11859    219619_at      0.120927\n",
      "13313    221909_at      0.111966\n",
      "37619    230272_at      0.101641\n",
      "\n",
      "Sample 10: Predicted Probability (Cancer - Class 1): 0.8800\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.204706\n",
      "49908  223313_s_at      0.171482\n",
      "13313    221909_at      0.158960\n",
      "9660     239082_at      0.154472\n",
      "730      205103_at      0.137205\n",
      "35407    227933_at      0.133173\n",
      "1563     244403_at      0.122485\n",
      "45685    223642_at      0.118035\n",
      "40879  218899_s_at      0.110504\n",
      "42470    227612_at      0.103360\n",
      "\n",
      "Sample 11: Predicted Probability (Cancer - Class 1): 0.9300\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.197988\n",
      "730      205103_at      0.197842\n",
      "40879  218899_s_at      0.177187\n",
      "35407    227933_at      0.169206\n",
      "9660     239082_at      0.168485\n",
      "49908  223313_s_at      0.164385\n",
      "13313    221909_at      0.156252\n",
      "11859    219619_at      0.153247\n",
      "1563     244403_at      0.131599\n",
      "28738    235238_at      0.105717\n",
      "\n",
      "Sample 12: Predicted Probability (Cancer - Class 1): 0.9600\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.201155\n",
      "730      205103_at      0.199582\n",
      "49908  223313_s_at      0.180336\n",
      "40879  218899_s_at      0.179127\n",
      "13313    221909_at      0.169223\n",
      "35407    227933_at      0.167054\n",
      "9660     239082_at      0.145238\n",
      "1563     244403_at      0.135840\n",
      "28738    235238_at      0.131562\n",
      "37619    230272_at      0.112854\n",
      "\n",
      "Sample 13: Predicted Probability (Cancer - Class 1): 0.9900\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "730      205103_at      0.187078\n",
      "4164     212233_at      0.180531\n",
      "40879  218899_s_at      0.161684\n",
      "49908  223313_s_at      0.154630\n",
      "9660     239082_at      0.141089\n",
      "1563     244403_at      0.125410\n",
      "13313    221909_at      0.120921\n",
      "35407    227933_at      0.116236\n",
      "11859    219619_at      0.107480\n",
      "37619    230272_at      0.099906\n",
      "\n",
      "Sample 14: Predicted Probability (Cancer - Class 1): 1.0000\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.204498\n",
      "730      205103_at      0.201913\n",
      "40879  218899_s_at      0.187212\n",
      "9660     239082_at      0.166862\n",
      "49908  223313_s_at      0.165579\n",
      "1563     244403_at      0.152913\n",
      "35407    227933_at      0.148954\n",
      "13313    221909_at      0.146528\n",
      "28738    235238_at      0.123790\n",
      "11859    219619_at      0.115848\n",
      "\n",
      "Sample 15: Predicted Probability (Cancer - Class 1): 0.9200\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.206565\n",
      "49908  223313_s_at      0.176837\n",
      "9660     239082_at      0.165557\n",
      "13313    221909_at      0.152354\n",
      "35407    227933_at      0.142103\n",
      "1563     244403_at      0.139470\n",
      "730      205103_at      0.132257\n",
      "45685    223642_at      0.122229\n",
      "40879  218899_s_at      0.121647\n",
      "11859    219619_at      0.116511\n",
      "\n",
      "Sample 16: Predicted Probability (Cancer - Class 1): 0.9000\n",
      "Top Contributing Features for This Sample:\n",
      "           Feature  Contribution\n",
      "4164     212233_at      0.188254\n",
      "9660     239082_at      0.170983\n",
      "730      205103_at      0.169530\n",
      "40879  218899_s_at      0.167603\n",
      "1563     244403_at      0.164200\n",
      "49908  223313_s_at      0.138989\n",
      "35407    227933_at      0.138208\n",
      "13313    221909_at      0.123574\n",
      "45685    223642_at      0.112398\n",
      "36483    209168_at      0.100324\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Predict probabilities for the test dataset\n",
    "y_pred = pipeline.predict(X_test)  # Predicted class (0 or 1)\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "\n",
    "# Extract feature importances from RandomForestClassifier\n",
    "feature_importances = pipeline.named_steps['classifier'].feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Loop through samples predicted as cancer (y = 1)\n",
    "for i, (pred, prob) in enumerate(zip(y_pred, y_pred_prob)):\n",
    "    if pred == 1:  # Only consider predictions for class 1 (cancer)\n",
    "        sample_features = X_test.iloc[i, :]  # Feature values for the sample\n",
    "        contributions = sample_features.values * feature_importances  # Calculate scaled contributions\n",
    "        \n",
    "        # Create a DataFrame for contributions\n",
    "        contribution_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Contribution': contributions\n",
    "        }).sort_values(by='Contribution', ascending=False)  # Sort by contribution\n",
    "        \n",
    "        # Print details for the sample\n",
    "        print(f\"\\nSample {i}: Predicted Probability (Cancer - Class 1): {prob:.4f}\")\n",
    "        print(\"Top Contributing Features for This Sample:\")\n",
    "        print(contribution_df.head(10))  # Display top 10 contributing features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brain.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "import joblib\n",
    "joblib.dump(pipeline, 'brain.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
